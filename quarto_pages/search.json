[
  {
    "objectID": "telemetry/index.html",
    "href": "telemetry/index.html",
    "title": "Ivy",
    "section": "",
    "text": "We want to give Ivy‚Äôs users as much flexibility as possible when it comes to data collection. Therefore, there are five different data sharing modes for you to choose from. üòÑ\nKeep in mind that any collected data is string-based and as concise as possible, this is, no training data, model parameter, or input array to your function or model will leave your computer at any time in terms of telemetry.\nAll of the data is totally anonymised, and is used to inform about Ivy‚Äôs usage, helping us prioritize the development of features and the improvement of the most used functions across all the versions of all the frameworks when it comes to compilation and transpilation.\nIvy will be a much better framework if it‚Äôs possible to prioritize adding new functions and improving existing ones based on their actual usage in the community, rather than leaving this all to guess work and potentially focusing our efforts on functions people don‚Äôt actually want or need.\nMore specifically, by sharing your data, this means more time will be spent on improving the functions, frameworks, and computation graphs that matter most for your projects.\nIf you use the premium features with an account, this data is stored in a database with the key being your account id. If you do not create an Ivy account, the data is stored in a database with the key being a securely hashed version of the IP address from where the call was made.\n\n\nThis section outlines the different modes available to all users, as well as the data that is collected for each one when calling ivy.compile, ivy.transpile, or ivy.unify.\n\nstealth\n\nNo data at all. üòÑ\nThis mode is limited to users with the enterprise license only.\n\napi-calls\n\nIvy function signature and its timestamp.\nAs an example, the message produced by this function call would be:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"}\n}\nThis is helpful to understand the typical usage of Ivy, making it possible to focus the development on the more widely used features and use cases of Ivy.\n\nfunctional-api\n\nIvy function signature, timestamp, and the set of used functions from the corresponding functional API, without their frequencies.\nA sample message in this case would be similar to:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"},\n    \"functions\": [\"torch.add\", \"torch.matmul\"]\n}\nAs mentioned above, this helps prioritize the addition of new functions and fixing of existing ones based on actual user usage of them, rather than relying on guess work and potentially focusing our efforts on functions that may not be wanted or needed by the users.\n\nfunctional-api-frequency\n\nIvy function signature, timestamp, and the set of used functions from the corresponding functional API along the number of calls to each function.\nA sample message if this mode is selected would be:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"},\n    \"functions\": [[\"torch.add\", 2], [\"torch.matmul\", 1]]\n}\nSimilar to the previous mode, this helps even more to prioritize certain functions that need to be included, fixed, or optimized based on their actual usage. The usage frequencies give another very helpful data point to be even more accurate with function prioritization, as it‚Äôs now possible to focus on the functions depending on how commonly used they are.\nNOTE: Users with the early pilot access should have this mode turned on at the minimum. This helps in informing the team about how Ivy is being used and where it can be improved. Thus, we ask for more usage data in order to improve our product at a faster pace.\n\ngraph\n\nIvy function signature, timestamp and a text-based representation of the computation graph. This mode is set by default on pip install ivy.\nIf this mode is selected, telemetry messages will be similar to:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"},\n    \"graph\": \"&lt;graph_representation&gt;\"\n}\nwhere &lt;graph_representation&gt; would be a string containing:\np708217 = args[0]\np862415 = args[1]\np817145 = torch.add(p708217, p862415)\np913484 = torch.add(p708217, p817145)\np868273 = torch.matmul(p913484, p817145)\nreturn p868273\nIn this case, telemetry helps not only with the function-level prioritization, but also with sub-function fixes and optimizations in Ivy‚Äôs functional API and Ivy‚Äôs frontends, as it‚Äôs now possible to distinguish which parameters or flags are used more often by users of the compiler and the transpiler.\nFurthermore, having an anonymized, minimal version of the computational graph allows us to recreate and explore any error that arises during compilation or transpilation, which once again enables the prioritization of bug fixes and features based on real-world usage.\nBelow, you can find a summary of the modes and their corresponding data levels:\n\n\n\n\n\n\n\n\n\n\n\n\nstealth\napi-calls\nfunctional-api\nfunctional-api-frequency\ngraph\n\n\n\n\nIvy signature\n-\nx\nx\nx\nx\n\n\nTime stamp\n-\nx\nx\nx\nx\n\n\nFunctions\n-\n-\nx\nx\nx\n\n\nFrequency\n-\n-\n-\nx\nx\n\n\nGraph repr.\n-\n-\n-\n-\nx\n\n\n\n\n\n\nBy default, the telemetry mode is graph. To change this setting, you should modify the config.json file that you can find inside of your .ivy directory. More specifically, you will have to change the data_level field. For example, if you want to limit the telemetry to avoid sending the computational graph, you can modify config.json so that it looks like this:\n{\n    \"data_level\": \"functional-api-frequency\"\n}"
  },
  {
    "objectID": "telemetry/index.html#telemetry-policy",
    "href": "telemetry/index.html#telemetry-policy",
    "title": "Ivy",
    "section": "",
    "text": "We want to give Ivy‚Äôs users as much flexibility as possible when it comes to data collection. Therefore, there are five different data sharing modes for you to choose from. üòÑ\nKeep in mind that any collected data is string-based and as concise as possible, this is, no training data, model parameter, or input array to your function or model will leave your computer at any time in terms of telemetry.\nAll of the data is totally anonymised, and is used to inform about Ivy‚Äôs usage, helping us prioritize the development of features and the improvement of the most used functions across all the versions of all the frameworks when it comes to compilation and transpilation.\nIvy will be a much better framework if it‚Äôs possible to prioritize adding new functions and improving existing ones based on their actual usage in the community, rather than leaving this all to guess work and potentially focusing our efforts on functions people don‚Äôt actually want or need.\nMore specifically, by sharing your data, this means more time will be spent on improving the functions, frameworks, and computation graphs that matter most for your projects.\nIf you use the premium features with an account, this data is stored in a database with the key being your account id. If you do not create an Ivy account, the data is stored in a database with the key being a securely hashed version of the IP address from where the call was made.\n\n\nThis section outlines the different modes available to all users, as well as the data that is collected for each one when calling ivy.compile, ivy.transpile, or ivy.unify.\n\nstealth\n\nNo data at all. üòÑ\nThis mode is limited to users with the enterprise license only.\n\napi-calls\n\nIvy function signature and its timestamp.\nAs an example, the message produced by this function call would be:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"}\n}\nThis is helpful to understand the typical usage of Ivy, making it possible to focus the development on the more widely used features and use cases of Ivy.\n\nfunctional-api\n\nIvy function signature, timestamp, and the set of used functions from the corresponding functional API, without their frequencies.\nA sample message in this case would be similar to:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"},\n    \"functions\": [\"torch.add\", \"torch.matmul\"]\n}\nAs mentioned above, this helps prioritize the addition of new functions and fixing of existing ones based on actual user usage of them, rather than relying on guess work and potentially focusing our efforts on functions that may not be wanted or needed by the users.\n\nfunctional-api-frequency\n\nIvy function signature, timestamp, and the set of used functions from the corresponding functional API along the number of calls to each function.\nA sample message if this mode is selected would be:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"},\n    \"functions\": [[\"torch.add\", 2], [\"torch.matmul\", 1]]\n}\nSimilar to the previous mode, this helps even more to prioritize certain functions that need to be included, fixed, or optimized based on their actual usage. The usage frequencies give another very helpful data point to be even more accurate with function prioritization, as it‚Äôs now possible to focus on the functions depending on how commonly used they are.\nNOTE: Users with the early pilot access should have this mode turned on at the minimum. This helps in informing the team about how Ivy is being used and where it can be improved. Thus, we ask for more usage data in order to improve our product at a faster pace.\n\ngraph\n\nIvy function signature, timestamp and a text-based representation of the computation graph. This mode is set by default on pip install ivy.\nIf this mode is selected, telemetry messages will be similar to:\ncomp_fn = ivy.compile(fn, to=‚Äùtorch‚Äù, include_generators=True)\n{\n    \"function\": ivy.compile,\n    \"timestamp\": 108586,\n    \"flags\": {\"to\": \"torch\", \"include_generators\": \"True\"},\n    \"graph\": \"&lt;graph_representation&gt;\"\n}\nwhere &lt;graph_representation&gt; would be a string containing:\np708217 = args[0]\np862415 = args[1]\np817145 = torch.add(p708217, p862415)\np913484 = torch.add(p708217, p817145)\np868273 = torch.matmul(p913484, p817145)\nreturn p868273\nIn this case, telemetry helps not only with the function-level prioritization, but also with sub-function fixes and optimizations in Ivy‚Äôs functional API and Ivy‚Äôs frontends, as it‚Äôs now possible to distinguish which parameters or flags are used more often by users of the compiler and the transpiler.\nFurthermore, having an anonymized, minimal version of the computational graph allows us to recreate and explore any error that arises during compilation or transpilation, which once again enables the prioritization of bug fixes and features based on real-world usage.\nBelow, you can find a summary of the modes and their corresponding data levels:\n\n\n\n\n\n\n\n\n\n\n\n\nstealth\napi-calls\nfunctional-api\nfunctional-api-frequency\ngraph\n\n\n\n\nIvy signature\n-\nx\nx\nx\nx\n\n\nTime stamp\n-\nx\nx\nx\nx\n\n\nFunctions\n-\n-\nx\nx\nx\n\n\nFrequency\n-\n-\n-\nx\nx\n\n\nGraph repr.\n-\n-\n-\n-\nx\n\n\n\n\n\n\nBy default, the telemetry mode is graph. To change this setting, you should modify the config.json file that you can find inside of your .ivy directory. More specifically, you will have to change the data_level field. For example, if you want to limit the telemetry to avoid sending the computational graph, you can modify config.json so that it looks like this:\n{\n    \"data_level\": \"functional-api-frequency\"\n}"
  },
  {
    "objectID": "privacy/index.html",
    "href": "privacy/index.html",
    "title": "Ivy",
    "section": "",
    "text": "This Privacy Policy explains how we collect, use, and disclose information when you use our website or services that utilize Google OAuth.\n\n\nWhen you use our website or services that utilize Google OAuth, we may collect certain information from your Google account, including your name, email address, and profile picture. This information is used to authenticate your account and provide you with access to our website or services.\n\n\n\nWe may use the information we collect from your Google account to provide you with the services you requested and to improve the overall user experience. We may also use your information to send you updates or notifications about our website or services.\n\n\n\nWe will not share, sell, or otherwise disclose the information we collect from your Google account to any third parties except as required by law or to protect the rights and safety of our website or services. The privacy policy and in-product privacy notifications clearly describe the way our application accesses, uses, stores, or shares Google user data.\n\n\n\nWe take the security of your information seriously and have implemented measures to protect it from unauthorized access or use. However, please note that no website or service is completely secure and we cannot guarantee the complete security of your information.\n\n\n\nWe may update this Privacy Policy from time to time to reflect changes in our practices or legal requirements. If we make any material changes, we will notify you by email or by posting a notice on our website.\n\n\n\nIf you have any questions or concerns about this Privacy Policy or our collection, use, or disclosure of your information, please contact us at saas@unify.ai."
  },
  {
    "objectID": "privacy/index.html#privacy-policy",
    "href": "privacy/index.html#privacy-policy",
    "title": "Ivy",
    "section": "",
    "text": "This Privacy Policy explains how we collect, use, and disclose information when you use our website or services that utilize Google OAuth.\n\n\nWhen you use our website or services that utilize Google OAuth, we may collect certain information from your Google account, including your name, email address, and profile picture. This information is used to authenticate your account and provide you with access to our website or services.\n\n\n\nWe may use the information we collect from your Google account to provide you with the services you requested and to improve the overall user experience. We may also use your information to send you updates or notifications about our website or services.\n\n\n\nWe will not share, sell, or otherwise disclose the information we collect from your Google account to any third parties except as required by law or to protect the rights and safety of our website or services. The privacy policy and in-product privacy notifications clearly describe the way our application accesses, uses, stores, or shares Google user data.\n\n\n\nWe take the security of your information seriously and have implemented measures to protect it from unauthorized access or use. However, please note that no website or service is completely secure and we cannot guarantee the complete security of your information.\n\n\n\nWe may update this Privacy Policy from time to time to reflect changes in our practices or legal requirements. If we make any material changes, we will notify you by email or by posting a notice on our website.\n\n\n\nIf you have any questions or concerns about this Privacy Policy or our collection, use, or disclosure of your information, please contact us at saas@unify.ai."
  },
  {
    "objectID": "odsc/index.html",
    "href": "odsc/index.html",
    "title": "ODSC Ivy Demo",
    "section": "",
    "text": "First, let‚Äôs install Ivy and some dependencies üòÑ\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout f705efe7cb5d18df17ce6c1e20f04d0eb4933f48 && python3 -m pip install --user -e .\n!pip install dm-haiku\n!pip install kornia\n!pip install timm\n!pip install pyvis\n!pip install transformers\nexit()"
  },
  {
    "objectID": "odsc/index.html#ivy-as-a-framework",
    "href": "odsc/index.html#ivy-as-a-framework",
    "title": "ODSC Ivy Demo",
    "section": "Ivy as a Framework",
    "text": "Ivy as a Framework\nIn this introduction, we will cover the fundamentals of using Ivy to write your own framework-indepent and future-proof code!\nIf you are interested in exploring the theoretical aspects behind the contents of this notebook you can check out the Design and the Deep Dive sections of the documentation!\nFirst of all, let‚Äôs import Ivy\n\nimport ivy\n\n\nIvy Backend Handler\nWhen used as a ML framework, Ivy is esentially an abstraction layer that supports multiple frameworks as the backend. This means that any code written in Ivy can be executed in any of the supported frameworks, with Ivy managing the framework-specific data structures, functions, optimizations, quirks and perks under the hood.\nTo switch the backend, we can use the ivy.set_backend function and pass the appropriate framework as a string. This is the easiest way to interact with the Backend Handler submodule, which manages the current backend and links Ivy‚Äôs objects and functions with the corresponding framework-specific ones.\nFor example:\n\nivy.set_backend(\"tensorflow\")\n\n\n\nData Structures\nThe basic data structure in Ivy is the ivy.Array. This is an abstraction of the array classes of the supported frameworks. Likewise, we also have ivy.NativeArray, which is an alias for the array class of the selected backend.\nLastly, there is another structure called the ivy.Container. It‚Äôs a subclass of dict that is optimized for recursive operations. If you want to learn more about it, you can defer to the following link!\nLet‚Äôs create an array using ivy.array(). Similarly, we can use ivy.native_array() to create a torch.Tensor now that the backend is set to torch.\n\nivy.set_backend(\"torch\")\n\nx = ivy.array([1, 2, 3])\nprint(type(x))\n\nx = ivy.native_array([1, 2, 3])\nprint(type(x))\n\n\n\nIvy Functional API\nIvy does not implement its own low-level (C++/CUDA) backend for its functions. Instead, it wraps the functional API of existing frameworks, unifying their fundamental functions under a common signature. For example, let‚Äôs take a look at ivy.matmul():\n\nivy.set_backend(\"jax\")\nx1, x2 = ivy.array([[1.], [2.], [3.]]), ivy.array([[1., 2., 3.]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nivy.set_backend(\"tensorflow\")\nx1, x2 = ivy.array([[1.], [2.], [3.]]), ivy.array([[1., 2., 3.]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nivy.set_backend(\"torch\")\nx1, x2 = ivy.array([[1.], [2.], [3.]]), ivy.array([[1., 2., 3.]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nThe output arrays shown above are ivy.Array instances. To obtain the underlying native array, we need to use the to_native() method.\nHowever, if you want the functions to return the native arrays directly, you can disable the array_mode of Ivy using ivy.set_array_mode().\n\nivy.set_array_mode(False)\n\nivy.set_backend(\"jax\")\nx1, x2 = ivy.native_array([[1.], [2.], [3.]]), ivy.native_array([[1., 2., 3.]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_backend(\"tensorflow\")\nx1, x2 = ivy.native_array([[1.], [2.], [3.]]), ivy.native_array([[1., 2., 3.]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_backend(\"torch\")\nx1, x2 = ivy.native_array([[1.], [2.], [3.]]), ivy.native_array([[1., 2., 3.]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_array_mode(True)\n\nKeeping this in mind, you can build any function you want as a composition of Ivy functions. When executed, this function will ultimately call the current backend functions from its functional API.\n\ndef sigmoid(z):\n    return ivy.divide(1, (1 + ivy.exp(-z)))\n\nIn essence, this means that by writing your code just once with Ivy, it becomes accessible for for use within any project regardless of the underlying framework being used!\n\n\nIvy Stateful API\nAs we have seen in the slides, Ivy also has a stateful API which builds on its functional API and the ivy.Container class to provide high-level classes such as optimizers, network layers, or trainable modules.\nThe most important stateful class within Ivy is ivy.Module, which can be used to create trainable layers and entire networks. A very simple example of an ivy.Module could be:\n\nclass Regressor(ivy.Module):\n    def __init__(self, input_dim, output_dim):\n        self.linear0 = ivy.Linear(input_dim, 128)\n        self.linear1 = ivy.Linear(128, output_dim)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = self.linear0(x)\n        x = ivy.functional.relu(x)\n        x = self.linear1(x)\n        return x\n\nTo use this model, we would simply have to set a backend and instantiate the model:\n\nivy.set_backend('torch')  # set backend to PyTorch\n\nmodel = Regressor(input_dim=1, output_dim=1)\noptimizer = ivy.Adam(0.1)\n\nNow we can generate some sample data and train the model using Ivy as well.\n\nn_training_examples = 2000\nnoise = ivy.random.random_normal(shape=(n_training_examples, 1), mean=0, std=0.1)\nx = ivy.linspace(-6, 3, n_training_examples).reshape((n_training_examples, 1))\ny = 0.2 * x ** 2 + 0.5 * x + 0.1 + noise\n\n\ndef loss_fn(pred, target):\n    return ivy.mean((pred - target)**2)\n\nfor epoch in range(50):\n    # forward pass\n    pred = model(x)\n\n    # compute loss and gradients\n    loss, grads = ivy.execute_with_gradients(lambda v: loss_fn(pred, y), model.v)\n\n    # update parameters\n    model.v = optimizer.step(model.v, grads)\n\n    # print current loss\n    print(f'Epoch: {epoch + 1:2d} --- Loss: {ivy.to_numpy(loss).item():.5f}')\n\nprint('Finished training!')\n\n\n\nGraph Compiler\nWe have just explored how to create framework agnostic functions and models with Ivy. Nonetheless, due to the wrapping Ivy performs on top of native functions, there is a slight performance overhead introduced with each function call. To address this, we can use Ivy‚Äôs graph compiler.\nThe purpose of the Graph Compiler is to extract a fully functional, efficient graph composed only of functions from the corresponding functional APIs of the underlying framework (backend).\nOn top of using the Graph Compiler to remove the overhead introduced by Ivy, it can also be used with functions and modules written directly with a given framework. In this case, the GC will decompose any high-level API into a fully-functional graph of functions from said framework.\nAs an example, let‚Äôs write a simple normalize function using Ivy:\n\ndef normalize(x):\n    mean = ivy.mean(x)\n    std = ivy.std(x)\n    return ivy.divide(ivy.subtract(x, mean), std)\n\nTo compile this function, simply call ivy.compile(). To specify the underlying framework, you can pass the name of the framework as an argument using to. Otherwise, the current backend will be used by default.\n\nimport torch\nx0 = torch.tensor([1., 2., 3.])\nnormalize_comp = ivy.compile(normalize, to=\"torch\", args=(x0,))\n\nThis results in the following graph:\n\nfrom IPython.display import HTML\nnormalize_comp.show(fname=\"graph.html\", notebook=True)\nHTML(filename=\"graph.html\")\n\nAs anticipated, the compiled function, which uses native torch operations directly, is faster than the original function:\n\n%%timeit\nnormalize(x0)\n\n\n%%timeit\nnormalize_comp(x0)\n\nAdditionally, we can set the return_backend_compiled_fn arg to True to apply the (native) target framework compilation function to Ivy‚Äôs compiled graph, making the resulting function even more efficient.\n\nnormalize_native_comp = ivy.compile(normalize, return_backend_compiled_fn=True, to=\"torch\", args=(x0,))\n\n\n%%timeit\nnormalize_native_comp(x0)\n\nIn the example above, we compiled the function eagerly, which means that the compilation process happened immediately, as we have passed the arguments for tracing. However, if we don‚Äôt pass any arguments to the compile function, compilation will occur lazily, and the graph will be built only when we call the compiled function for the first time. To summarize:\n\nimport torch\n\nx1 = torch.tensor([1., 2., 3.])\n\n\n# Arguments are available -&gt; compilation happens eagerly\neager_graph = ivy.compile(normalize, to=\"torch\", args=(x1,))\n\n# eager_graph is now torch code and runs efficiently\nret = eager_graph(x1)\n\n\n# Arguments are not available -&gt; compilation happens lazily\nlazy_graph = ivy.compile(normalize, to=\"torch\")\n\n# The compiled graph is initialized, compilation will happen here\nret = lazy_graph(x1)\n\n# lazy_graph is now torch code and runs efficiently\nret = lazy_graph(x1)"
  },
  {
    "objectID": "odsc/index.html#ivy-as-a-transpiler",
    "href": "odsc/index.html#ivy-as-a-transpiler",
    "title": "ODSC Ivy Demo",
    "section": "Ivy as a Transpiler",
    "text": "Ivy as a Transpiler\nWe have just learned how to write framework-agnostic code and compile it into an efficient graph. However, many codebases, libraries, and models have already been developed (and will continue to be!) using other frameworks.\nTo allow for speed-of-thought research and development, Ivy also allows you to use any code directly into your project, regardless of the framework it was written in. No matter what ML code you want to use, Ivy‚Äôs Transpiler is the tool for the job üõ†Ô∏è\n\nAny function\nLet‚Äôs start by transpiling a very simple torch function.\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\njax_normalize = ivy.transpile(normalize, source=\"torch\", to=\"jax\")\n\nSimilar to compile, the transpile function can be used eagerly or lazily. In this particular example, transpilation is being performed lazily, since we haven‚Äôt passed any arguments or keyword arguments to ivy.transpile.\n\nimport jax\nkey = jax.random.PRNGKey(42)\njax.config.update('jax_enable_x64', True)\nx = jax.random.uniform(key, shape=(10,))\n\njax_out = jax_normalize(x)\nprint(jax_out, type(jax_out))\n\nThat‚Äôs pretty much it! You can now use any function you need in your projects regardless of the framework you‚Äôre using üöÄ\nHowever, transpiling functions one by one is far from ideal. But don‚Äôt worry, with transpile, you can transpile entire libraries at once and easily bring them into your projects. Let‚Äôs see how this works by transpiling kornia, a wisely-used computer vision library written in torch:\n\n\nAny library\n\nimport kornia\nimport requests\nimport jax.numpy as jnp\nimport numpy as np\nfrom PIL import Image\n\nLet‚Äôs get the transpiled library by calling transpile.\n\njax_kornia = ivy.transpile(kornia, source=\"torch\", to=\"jax\")\n\nNow let‚Äôs get a sample image and preprocess so that it has the format kornia expects:\n\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\nraw_img = Image.open(requests.get(url, stream=True).raw)\nimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))\nimg = jnp.expand_dims(img, 0) / 255\ndisplay(raw_img)\n\nAnd we can call any function from kornia in jax, as simple as that!\n\nout = jax_kornia.enhance.sharpness(img, 10)\ntype(out)\n\nFinally, let‚Äôs see if the transformation has been applied correctly:\n\nnp_image = np.uint8(np.array(out[0])*255)\ndisplay(Image.fromarray(np.transpose(np_image, (1, 2, 0))))\n\nIt‚Äôs worth noting that every operation in the transpiled functions is performed natively in the target framework, which means that gradients can be tracked and the resulting functions are fully differentiable. Even after transpilation, you can still take advantage of the powerful features of your chosen framework.\nWhile transpiling functions and libraries is useful, trainable modules play a critical role in ML and DL. The good news is that Ivy makes it just as easy to transpile modules and models from one framework to another with just one line of code.\n\n\nAny model\nFor the purpose of this demonstration, let‚Äôs define a very basic CNN block using the Sequential API of keras.\n\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 3)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nThe model we just defined is an instance of tf.keras.Model. Using ivy.transpile, we can effortlessly convert it into a torch.nn.Module, for instance.\n\ninput_array = tf.random.normal((1, 28, 28, 3))\ntorch_model = ivy.transpile(model, to=\"torch\", args=(input_array,))\n\nAfter transpilation, we can pass a torch tensor and obtain the expected output. As mentioned previously, all operations are now PyTorch native functions, making them differentiable. Additionally, Ivy automatically converts all parameters of the original model to the new one, allowing you to transpile pre-trained models and fine-tune them in your preferred framework.\n\nisinstance(torch_model, torch.nn.Module)\n\n\ninput_array = torch.rand((1, 28, 28, 3)).to(ivy.default_device(as_native=\"True\"))\ntorch_model.to(ivy.default_device(as_native=\"True\"))\noutput_array = torch_model(input_array)\nprint(output_array)\n\nWhile we have only transpiled a simple model for demonstration purposes, we can certainly transpile more complex models as well. Let‚Äôs take a more complex model from timm and see how we can build upon transpiled modules.\n\nimport timm\n\nWe will only be using the encoder, so we can remove the unnecessary layers by setting num_classes=0, and then pass pretrained=True to download the pre-trained parameters.\n\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\n\nLet‚Äôs transpile the model to tensorflow with ivy.transpile üîÄ\n\nnoise = torch.randn(1, 3, 224, 224)\ntf_mlp_encoder = ivy.transpile(mlp_encoder, to=\"tensorflow\", args=(noise,))\n\nAnd now let‚Äôs build a model on top of our pretrained encoder!\n\nclass Classifier(tf.keras.Model):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.encoder = tf_mlp_encoder\n        self.output_dense = tf.keras.layers.Dense(units=1000, activation=\"softmax\")\n\n    def call(self, x):\n        x = self.encoder(x)\n        return self.output_dense(x)\n\n\nmodel = Classifier()\n\nx = tf.random.normal(shape=(1, 3, 224, 224))\nret = model(x)\nprint(type(ret), ret.shape)\n\nAs the encoder now consists of tensorflow functions, we can extend the transpiled modules as much as we want, leveraging existing weights and the tools and infrastructure of all frameworks üöÄ\nLast but not least, let‚Äôs see how easily we can improve the performance of a model by transpiling a Vision Transformer from Hugging Face from PyTorch to JAX ‚¨áÔ∏è\nFirst we need to load the model and its corresponding feature extractor from the transformers library.\n\nimport jax\nfrom transformers import AutoModel, AutoFeatureExtractor\n\njax.config.update(\"jax_enable_x64\", True)\n\narch_name = \"Vision Transformer (ViT)\"\ncheckpoint_name = \"google/vit-base-patch16-224\"\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(checkpoint_name)\nmodel = AutoModel.from_pretrained(checkpoint_name)\n\nNow let‚Äôs download a sample image from the COCO dataset and use the feature extractor we‚Äôve just created to generate the torch tensors we‚Äôll be using during tracing\n\nimport requests\nfrom PIL import Image\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(\n    images=image, return_tensors=\"pt\"\n)\n\nWe can now convert the model from torch to haiku simply calling ivy.transpile()!\n\ntranspiled_graph = ivy.transpile(model, to=\"haiku\", kwargs=inputs)\n\nAfter transpiling the model, let‚Äôs see what the improvement in runtime efficiency looks like. For this we‚Äôll compile the original PyTorch model using torch.compile\n\nimport torch\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\").to(\"cuda\")\n\nmodel.to(\"cuda\")\n\ndef _f(**kwargs):\n  return model(**kwargs)\n\ncomp_model = torch.compile(_f)\n_ = comp_model(**inputs)\n\nAnd the equivalent compilation of our haiku model with jax.jit\n\nimport haiku as hk\n\ninputs_jax = feature_extractor(images=image, return_tensors=\"jax\")\n\ndef _forward(**kwargs):\n  module = transpiled_graph()\n  return module(**kwargs).last_hidden_state\n\n_forward = jax.jit(_forward)\nrng_key = jax.random.PRNGKey(42)\njax_mlp_forward = hk.transform(_forward)\nparams = jax_mlp_forward.init(rng=rng_key, **inputs_jax)\n\nNow that both models are compiled in their corresponding frameworks, let‚Äôs see how their runtime speeds compare to each other:\n\n%%timeit\n_ = comp_model(**inputs)\n\n\n%%timeit\nout = jax_mlp_forward.apply(params, None, **inputs_jax)\n\nAs expected, we have made the model significantly faster with just one line of code, getting a ~3x increase in its execution speed üöÄ\nFinally, as a sanity check, let‚Äôs load a different image and make sure that the results are the same in both models\n\nurl = \"http://images.cocodataset.org/train2017/000000283921.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=\"pt\").to(\"cuda\")\ninputs_jax = feature_extractor(images=image, return_tensors=\"jax\")\nout_torch = comp_model(**inputs)\nout_jax = jax_mlp_forward.apply(params, None, **inputs_jax)\n\nnp.allclose(out_torch.last_hidden_state.detach().cpu().numpy(), out_jax, atol=1e-4)\n\nThat‚Äôs pretty much it! The results from both models are the same, but we have achieved a solid speed up by using Ivy‚Äôs transpiler to convert the model to JAX!"
  }
]